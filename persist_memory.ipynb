{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Intialize your llm </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt432\",\n",
    "    openai_api_key=os.getenv('CHAT_API_KEY'),\n",
    "    azure_endpoint= os.getenv('CHAT_ENDPOINT'),\n",
    "    openai_api_version=os.getenv(\"CHAT_API_VERSION\")\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm= llm, memory_key=\"chat_history\", max_token_limit=100, input_key='human_input', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Given a chat history and the latest user question which might reference context in the chat extract_memory \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "chat_extract_memory: {chat_history}\n",
    "user_question: {human_input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt= prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant for question-answering tasks. Given a chat history and the latest user question which might reference context in the chat extract_memory Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.chat_extract_memory: []\n",
      "user_question: How about telling me a joke to start things off?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"How about telling me a joke to start things off?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant for question-answering tasks. Given a chat history and the latest user question which might reference context in the chat extract_memory Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.chat_extract_memory: [HumanMessage(content='How about telling me a joke to start things off?'), AIMessage(content=\"Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything!\")]\n",
      "user_question:  Can you provide information about famous scientists?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Absolutely, there are many famous scientists across different fields. For example, Albert Einstein is renowned for his theory of relativity in physics. In biology, Charles Darwin is known for his evolutionary theories. Marie Curie made significant contributions to physics and chemistry and is the only person to win a Nobel prize in both disciplines.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\" Can you provide information about famous scientists?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant for question-answering tasks. Given a chat history and the latest user question which might reference context in the chat extract_memory Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.chat_extract_memory: [SystemMessage(content='The human asks the AI to tell a joke to start things off. The AI responds with a light-hearted joke about scientists not trusting atoms because they make up everything.'), HumanMessage(content=' Can you provide information about famous scientists?'), AIMessage(content='Absolutely, there are many famous scientists across different fields. For example, Albert Einstein is renowned for his theory of relativity in physics. In biology, Charles Darwin is known for his evolutionary theories. Marie Curie made significant contributions to physics and chemistry and is the only person to win a Nobel prize in both disciplines.')]\n",
      "user_question:  what can you tell me about marie curie\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Marie Curie was a renowned physicist and chemist. She is well known for her research on radioactivity, and she was the first woman to win a Nobel Prize, the only woman to win in two fields, and the only person to win in multiple sciences. Her significant contributions were in the fields of Physics and Chemistry.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\" what can you tell me about marie curie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extracting and Passing memory to new chain </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [SystemMessage(content='The human asks the AI to tell a joke and then requests information about famous scientists. The AI responds with a joke about atoms and then provides examples of famous scientists like Albert Einstein, Charles Darwin, and Marie Curie, highlighting their significant contributions to their respective fields.'),\n",
       "  HumanMessage(content=' what can you tell me about marie curie'),\n",
       "  AIMessage(content='Marie Curie was a renowned physicist and chemist. She is well known for her research on radioactivity, and she was the first woman to win a Nobel Prize, the only woman to win in two fields, and the only person to win in multiple sciences. Her significant contributions were in the fields of Physics and Chemistry.')]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mem = memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to store and retrieve in database you can use pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passing to new chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_memory = ConversationSummaryBufferMemory(llm = llm, \n",
    "                                             max_token_limit=200, \n",
    "                                             memory_key='chat_extract_memory', \n",
    "                                             input_key='human_input', \n",
    "                                             return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_extract_memory': []}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in extract_mem['chat_history']:\n",
    "    if isinstance(item, SystemMessage):\n",
    "        blank_memory.moving_summary_buffer = item.content\n",
    "    elif isinstance(item, HumanMessage):\n",
    "        blank_memory.chat_memory.add_user_message(item.content)\n",
    "    elif isinstance(item, AIMessage):\n",
    "        blank_memory.chat_memory.add_ai_message(item.content)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_extract_memory': [SystemMessage(content='The human asks the AI to tell a joke and then requests information about famous scientists. The AI responds with a joke about atoms and then provides examples of famous scientists like Albert Einstein, Charles Darwin, and Marie Curie, highlighting their significant contributions to their respective fields.'),\n",
       "  HumanMessage(content=' what can you tell me about marie curie'),\n",
       "  AIMessage(content='Marie Curie was a renowned physicist and chemist. She is well known for her research on radioactivity, and she was the first woman to win a Nobel Prize, the only woman to win in two fields, and the only person to win in multiple sciences. Her significant contributions were in the fields of Physics and Chemistry.')]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
